# Ollama Context and Model Context

Verify:

- Model Context Window Restriction
- Ollama Context Window Restriction

## Model Context Window

### Option 1

```shell
docker exec -it b26021be4114 bash

root@b26021be4114:~# ollama run llama3.2:3b
>>> /show info
  Model
    architecture        llama
    parameters          3.2B
    context length      131072
    embedding length    3072
    quantization        Q4_K_M

  Capabilities
    completion
    tools

  Parameters
    stop    "<|start_header_id|>"
    stop    "<|end_header_id|>"
    stop    "<|eot_id|>"

  License
    LLAMA 3.2 COMMUNITY LICENSE AGREEMENT
    Llama 3.2 Version Release Date: September 25, 2024
    â€¦

>>> /bye
```

### Option 2

```shell
curl http://localhost:11434/api/show -d '{"name": "llama3.2:3b"}'
```

```json
{
  "license": "LLAMA 3.2 COMMUNITY LICENSE AGREEMENT...",
  "modelfile": "# Modelfile generated by \"ollama show\"\n...",
  "parameters": "...",
  "template": "...",
  "details": {
    "parent_model": "",
    "format": "gguf",
    "family": "llama",
    "families": [
      "llama"
    ],
    "parameter_size": "3.2B",
    "quantization_level": "Q4_K_M"
  },
  "model_info": {
    "general.architecture": "llama",
    "general.basename": "Llama-3.2",
    "general.file_type": 15,
    "general.finetune": "Instruct",
    "general.languages": null,
    "general.parameter_count": 3212749888,
    "general.quantization_version": 2,
    "general.size_label": "3B",
    "general.tags": null,
    "general.type": "model",
    "llama.attention.head_count": 24,
    "llama.attention.head_count_kv": 8,
    "llama.attention.key_length": 128,
    "llama.attention.layer_norm_rms_epsilon": 0.00001,
    "llama.attention.value_length": 128,
    "llama.block_count": 28,
    "llama.context_length": 131072,
    "llama.embedding_length": 3072,
    "llama.feed_forward_length": 8192,
    "llama.rope.dimension_count": 128,
    "llama.rope.freq_base": 500000,
    "llama.vocab_size": 128256,
    "tokenizer.ggml.bos_token_id": 128000,
    "tokenizer.ggml.eos_token_id": 128009,
    "tokenizer.ggml.merges": null,
    "tokenizer.ggml.model": "gpt2",
    "tokenizer.ggml.pre": "llama-bpe",
    "tokenizer.ggml.token_type": null,
    "tokenizer.ggml.tokens": null
  },
  "tensors": [],
  "capabilities": [
    "completion",
    "tools"
  ],
  "modified_at": "2025-08-21T11:01:44Z"
}
```

---

## Ollama Context Window Restriction

```shell
docker exec -it b26021be4114 bash
root@b26021be4114:~# ollama ps
NAME           ID              SIZE      PROCESSOR    CONTEXT    UNTIL
llama3.2:3b    a80c4f17acd5    3.3 GB    100% GPU     4096       4 minutes from now
```

## Ollama Run with bigger Context Window

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "your long text here...",
  "options": {
    "num_ctx": 131072
  }
}'

curl http://localhost:11434/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "model": "llama3.2:3b",
  "messages": [
    {
      "role": "user",
      "content": "your 20k char text here..."
    }
  ],
  "options": {
    "num_ctx": 8192
  }
}'
```
